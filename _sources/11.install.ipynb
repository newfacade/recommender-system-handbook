{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 安装Spark\n",
    "\n",
    "```{note}\n",
    "作为Python开发者，我们只要安装pyspark就可以了。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装pyspark\n",
    "\n",
    "```\n",
    "pip install pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证安装\n",
    "\n",
    "### 在命令行输入pyspark\n",
    "\n",
    "```\n",
    "(base) facer@bogon ~ % pyspark\n",
    "Python 3.8.3 (default, Jul  2 2020, 11:26:31)\n",
    "[Clang 10.0.0 ] :: Anaconda, Inc. on darwin\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.0.2\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.8.3 (default, Jul  2 2020 11:26:31)\n",
    "SparkSession available as 'spark'.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看spark版本\n",
    "\n",
    "```\n",
    ">>> spark.version\n",
    "'3.0.2'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取一个文件，看它一共有多少行\n",
    "\n",
    "“Hello, World” of distributed computing.\n",
    "\n",
    "```\n",
    ">>> strings = spark.read.text(\"README.md\")\n",
    ">>> strings.show(10, truncate=False)\n",
    "+--------------------------------------------------------------------------------+\n",
    "|value                                                                           |\n",
    "+--------------------------------------------------------------------------------+\n",
    "|# Apache Spark                                                                  |\n",
    "|                                                                                |\n",
    "|Spark is a unified analytics engine for large-scale data processing. It provides|\n",
    "|high-level APIs in Scala, Java, Python, and R, and an optimized engine that     |\n",
    "|supports general computation graphs for data analysis. It also supports a       |\n",
    "|rich set of higher-level tools including Spark SQL for SQL and DataFrames,      |\n",
    "|MLlib for machine learning, GraphX for graph processing,                        |\n",
    "|and Structured Streaming for stream processing.                                 |\n",
    "|                                                                                |\n",
    "|<https://spark.apache.org/>                                                     |\n",
    "+--------------------------------------------------------------------------------+\n",
    "only showing top 10 rows\n",
    "\n",
    ">>> strings.count()\n",
    "108\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
