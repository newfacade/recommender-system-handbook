{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark简介\n",
    "\n",
    "```{note}\n",
    "Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎。<br/>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop\n",
    "\n",
    "Hadoop是一个能够对大量数据进行分布式处理的软件框架。\n",
    "\n",
    "Hadoop框架最核心的设计就是：HDFS和MapReduce。\n",
    "\n",
    "HDFS为海量的数据提供了存储，而MapReduce则为海量的数据提供了计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark的特质\n",
    "\n",
    "MapReduce有如下缺点：\n",
    "\n",
    "1.很复杂<br/>\n",
    "2.低容错<br/>\n",
    "3.频繁磁盘I/O速度慢\n",
    "\n",
    "针对这些缺点，Spark诞生了，它拥有如下的特质：\n",
    "\n",
    "1.速度快，Spark会把计算过程抽象成DAG并依此优化，中间结果尽量放在内存避免磁盘I/O<br/>\n",
    "2.易于使用，就像使用pandas的DataFrame<br/>\n",
    "3.模块化<br/>\n",
    "4.可扩展性\n",
    "\n",
    "Spark的各模块和API：\n",
    "\n",
    "![image](images/spark1.jpg)\n",
    "\n",
    "```{tip}\n",
    "Spark SQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame并且作为分布式SQL查询引擎的作用。<br>\n",
    "我们重点关注Spark SQL，使用Python语言的API。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark的分布式计算\n",
    "\n",
    "Spark计算时的总体结构如下：\n",
    "\n",
    "![image](images/compute.jpg)\n",
    "\n",
    "Spark Application: 我们的程序\n",
    "\n",
    "Spark Driver: 计算DAG图等\n",
    "\n",
    "Spark Session: 使用Spark的入口\n",
    "\n",
    "Cluster Manager: 集群管理器\n",
    "\n",
    "Spark Executor: 打工人"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
